{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import ner_training, expansion, seed_data_extraction\n",
    "from postprocessing import training_data_generation, extract_new_entities, filtering\n",
    "import config as cfg\n",
    "import gensim\n",
    "import elasticsearch\n",
    "from config import ROOTPATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es = elasticsearch.Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "model_doc2vec = gensim.models.Doc2Vec.load(cfg.ROOTPATH + '/models/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seeds = ['clueweb', 'imagenet', 'flickr', 'webkb', 'netflix', 'imdb']\n",
    "context_words = ['dataset', 'corpus', 'collection', 'repository', 'benchmark', 'website']\n",
    "sentence_expansion = True\n",
    "training_cycles = 2\n",
    "model_name = 'dataset'\n",
    "training_cycle = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for cycle in range(training_cycles):\n",
    "    print(cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#  seed_data_extraction.sentence_extraction(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training data extraction\n",
      "Selected seed terms ['clueweb', 'webkb', 'imagenet', 'netflix', 'flickr', 'imdb']\n",
      "clueweb\n",
      "Got 393 hits in ES\n",
      "webkb\n",
      "Got 124 hits in ES\n",
      "imagenet\n",
      "Got 41 hits in ES\n",
      "netflix\n",
      "Got 548 hits in ES\n",
      "flickr\n",
      "Got 2521 hits in ES\n",
      "imdb\n",
      "Got 448 hits in ES\n",
      "2546 sentences added for training in iteration\n",
      "\n",
      "Process finished with 6 seeds\n"
     ]
    }
   ],
   "source": [
    "seed_data_extraction.sentence_extraction(model_name, seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#  expansion.term_expansion_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started to extract generic named entity from sentences...\n",
      "Finished processing sentences with 5259 new possible entities\n",
      "started clustering\n",
      "......added 1548 expanded terms\n",
      ".........................................."
     ]
    }
   ],
   "source": [
    "expansion.term_expansion(model_name, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding sentences\n",
      "Added 2525 expanded sentences to the 4101 original\n"
     ]
    }
   ],
   "source": [
    "expansion.sentence_expansion(model_name, 0, model_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.doc2vec.Doc2Vec"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainingdata_generation.generate_training_term_expansion_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling\n",
      "4095 lines labelled\n"
     ]
    }
   ],
   "source": [
    "trainingdata_generation.generate_training_term_expansion_only(model_name, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainingdata_generation.label_sentences_term_sentence_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling\n",
      "4101 lines labelled\n"
     ]
    }
   ],
   "source": [
    "trainingdata_generation.sentence_labelling(model_name, 0, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ner_training.create_austenprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner_training.create_prop(model_name, 0, sentence_expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner_training.train_model(model_name, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started extraction..... dataset cycle 0\n",
      "815\n",
      "...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................797\n",
      ".............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................0\n",
      "0\n",
      "Total of 1751 filtered words added\n"
     ]
    }
   ],
   "source": [
    "extract_new_entities.ne_extraction(model_name, 0, sentence_expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering...\n",
      "Started  pmi filtering...\n",
      "Calculating npd with dataset\n",
      "!....!.....!....!.!.........!......!..!.....!....................................!...!...........!.....!........!....!......!...!.!................!.............................!.............!...!....!...!..!................!.....!..!...!........!......!..!.!.............!.....!..!............!..!......!...!.....!.....................!....!......!...........!.......!........!......!..............................!..!..!.....!.....!.....!..!..!...!..!..!.!....!.......!........!...!.......!.!.!.......!...!..!........!....!.......!.......!.!..!......!.....!...!.!.....!....!..!...............!........................!.....!.....!.........!.!..........!.....!........!.!....!...............!....!........!......!....!.!.!.!..!................!.!.....!.......!..............!................!................!..............!.!.!....!.....!.!.....!........!..!...!...!.!...........!......!..........!..!.....!....!.........!...........!..!........!.......!.!.....!....!..................!.........!..!.....!..........!.....!...!.!............!.........!.!....!.......!..............!..!.!...!...!..Calculating npd with corpus\n",
      "..!..!....!.!.!.!.!...!..!..........!.....!......!...!...........!.......!..!............!.............!....!..!.........!.....!.......!....!..!..........!..!.........!..........!.....!..!........!.......!...!....!.....!......!.!.....!.!.!...!..........!.....!..!.!.....!......!..!.................!.....!....!...!.!.!...........!....!.......!.....!..............!........!.............!........!...!.!........!..!.!...!.......!..!.........!....!......!....!..........!....!.....!....!.!...........!.....!..!....!.....!........!........!.................!..!.....!..!....................!........!...!.!...!..........!..!....!.....!...!.....!....!.!..!......!.!.!..........!...................................!.!.....!.....!.!......!..........!.........!..!.!......!......!...!....!..!...!.............!.....!...!..!.......................!...........!.....!...................!.......!.!.....!.!.........!.....!...!.....!...!....!..!....!.!......!..!.!....!.!.........!..!......!.....Calculating npd with collection\n",
      ".!.!...!.!.........!...!.!....!..............!.....!.!.......!........!........!.......!.......!..!....!...!..!...............!................!.....!.....!...!...!..!..!.....!....!....!.................!..!.............!...!..!...!.......!.!...........!.........................!.!.......!..........!.........!....!.......!........!......................!.............!...!....!....!...!....!......!..!.....!.......!.!.!...!....!.!......!.!....!...!.!.....................!.!............!..............!..............................!..............!.........................!....!............!....!...................!.....!.!.................!...!......!.!..............!....!..!.!...!..........!.............!.....................!.!..!..!..!......!...!.....!............!.........!.!..................!...!.........!.......!....!...!...!...........!.!......................!.!....!...!.............!...........!...!....!..........!.......!..!..........!...!.!...!.!.!..!......!.....!.....!...!......!.!...!..!..!....!.......................!.!.....!..............!........!....!...!..............!....!........!.....!.!........................!..!.!.!.......................!.!.!................!..!....!.............Calculating npd with repository\n",
      "!..!.!.....!..!.!.!........!....!....!....!.......!...!..!..!.!....................!......!......!.!.....!.!..!..!...!.....!.......!.!......!..!...!....!..!....!......!.!...!.!.....!.!..!.!..!....!.....!....!....!......!..!..!..!.!...!.......!...........!....!.....!.!.!.....!....!.......!....!.!.!........!.!..........!.........!.!.....!....!.!....!...!..!.!.!.!.!....!.!.!.......!......!..!..!..!.......!...!..!.....!....!....!........!.!....!....!....!.....!...........!........!.!....!..!.!.!.!......!..!..!..!....!..................!..!...!.!.....!.!..!..!.!..!.......!..!..........!....!...!..!..!....!.........!.!...!.!........!.........!.......!.....!.!.!...!.!...!.....!..!.....!.!.!..........!........!...!.!..!.....!..!..!....!.!.!.!..!..............!..!.!..!.!.....!...........!......!......!.!.........!.!.....!.....!..!....!.....!..............!..!....!....!...........!.!.....!..!....!...!....!.!........!.!..!.........!....!....!.....!....!.!..!...!.!...!........!...!....!...!.!.........Calculating npd with benchmark\n",
      "!..!.!...........!.......!.............!.............!..!......!....!.........!...!.....!...!.......................!........!.....!.....!.....!...!....!.....!.................!.....................!..!....!..!......!............!.......................!........!...........................!....!.....!....!................!.!.....!.................!................!.........!............!.....................................!.....!...........................!..........!..................!.............!.......!.......!.!.......!......!....!............!......................!......!...........................!..!....!..!.....!.................!.......!.!....!..........!.!..!.!..!............!..........Calculating npd with website\n",
      "!.!...!..!...!......!...!......!...!.!...!......!..!...............!...................!.....!...!......!.........!.....!.....!....!..!....!....!.....!..!.!..!....!......!..!..!..!.!.......!...!.!...!.......!...!.!......!.........!.........!....!.!.....!.!...!......!..!.................!...!...!......................!..!.!.......!...!......!......!.!.............!.!...!...!.....!......!....!...!................!.!.......!....!......!.....!.!.......!.!.!.......!.!.!.!..!.!........!.........!............!..!....!..!.!.....!...............!...!.......!.!.!..!..............!.....!.....!.......!..!....!..!.!......!......!...........!.!....!..!...!..!.......!..!..!...!..!.............!.....!.!.........!...............!.!...!.............!....!.!..!.!..!.!.....!.!.!......!...........!.!.!....!...!.!.......!.!.!.!..........!..!......!...!.....!...!.....!.!...!.!..!...........!.........!....!.!....!.....!..!....!...!...!......629 entities filtered from 1751\n"
     ]
    }
   ],
   "source": [
    "filtering.pmi(model_name, training_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy\n",
    "import string\n",
    "from numbers import Number\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from config import ROOTPATH\n",
    "from postprocessing import normalized_pub_distance\n",
    "\n",
    "stopword_path = ROOTPATH + \"/data/stopword_en.txt\"\n",
    "stopword_list = []\n",
    "with open(stopword_path, 'r') as file:\n",
    "        for sw in file.readlines():\n",
    "            stopword_list.append(sw.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 1854 with WordNet and Stopwords\n"
     ]
    }
   ],
   "source": [
    "extracted_entities = []\n",
    "path = ROOTPATH + '/processing_files/' + model_name + '_extracted_entities_' + str(training_cycle) + '.txt'\n",
    "with open(path, \"r\") as f:\n",
    "    for e in f.readlines():\n",
    "        extracted_entities.append(e.strip())\n",
    "print('Filtering', len(extracted_entities), 'with WordNet and Stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 1854 with WordNet and Stopwords\n"
     ]
    }
   ],
   "source": [
    "extracted_entities = []\n",
    "path = ROOTPATH + '/processing_files/' + model_name + '_extracted_entities_' + str(training_cycle) + '.txt'\n",
    "with open(path, \"r\") as f:\n",
    "    extracted_entities = [e.strip() for e in f.readlines()]\n",
    "print('Filtering', len(extracted_entities), 'with WordNet and Stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846\n",
      "1801\n"
     ]
    }
   ],
   "source": [
    "ws_filtered = []\n",
    "stopword_filtered = [word for word in set(extracted_entities) if word.lower() not in stopwords.words('english')]\n",
    "print(len(stopword_filtered))\n",
    "stopword_filtered = [word for word in set(stopword_filtered) if word.lower() not in stopword_list]\n",
    "print(len(stopword_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_by_wordnet = [word for word in stopword_filtered if not wordnet.synsets(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_by_wordnet = []\n",
    "for word in set(stopword_filtered):\n",
    "    if not wordnet.synsets(word):\n",
    "        filter_by_wordnet.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055\n"
     ]
    }
   ],
   "source": [
    "print(len(filter_by_wordnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055\n"
     ]
    }
   ],
   "source": [
    "print(len(filter_by_wordnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = ROOTPATH + '/processing_files/' + model_name + '_extracted_entities_' + str(training_cycle) + '.txt'\n",
    "with open(path, \"r\") as f:\n",
    "    extracted_entities = [e.strip().lower() for e in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 1854 entities with embedded clustering\n",
      "........716 entities are kept from the total of 1854\n"
     ]
    }
   ],
   "source": [
    "filtering.filter_st_pmi_kbl_ec(model_name, training_cycle, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 1854 entities with PMI\n",
      "678 entities are kept from the total of 1854\n"
     ]
    }
   ],
   "source": [
    "filtering.filter_pmi(model_name, training_cycle, context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 1854 entities by vote of selected filter methods\n",
      "598 entities are kept from the total of 1854\n"
     ]
    }
   ],
   "source": [
    "filtering.majority_vote(model_name, training_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import sys\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from config import ROOTPATH, STANFORD_NER_PATH\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'dataset'\n",
    "input_text = 'Knowledge-base construction (KBC) is the process of populating a knowledge base (KB) with facts (or assertions) extracted from text. It has recently received tremendous interest from academia, e.g., CMU’s NELL [2] and MPI’s YAGO [7,9], and from industry, e.g., IBM’s DeepQA [5] and Microsoft’s EntityCube [16]. To achieve high quality, these systems leverage a wide variety of data resources and KBC techniques. A crucial challenge that these systems face is coping with imperfect or conflicting information from multiple sources [3, 13]. To address this challenge, we present an end-to-end KBC system called DeepDive. 1 DeepDive went live in January 2012 after processing the 500M English web pages in the ClueWeb09 corpus2 , and since then has been adding several million newly-crawled webpages every day. Figure 1 shows several screenshots of DeepDive. Similar to YAGO [7,9] and EntityCube [16], DeepDive is based on the classic Entity-Relationship (ER) model [1] and employs popular techniques such as distant supervision [15] and the Markov logic language [11] to combine a variety of signals. However, DeepDive goes deeper in two ways: (1) Unlike prior large-scale KBC systems, DeepDive performs deep natural language processing (NLP) to extract useful'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started extraction for the dataset model\n",
      "Knowledge-base construction KBC is the process of populating a knowledge base KB with facts or assertions extracted from text.\n",
      ".It has recently received tremendous interest from academia  , e.g.  , CMU’s NELL  and MPI’s YAGO   , and from industry  , e.g.  , IBM’s DeepQA  and Microsoft’s EntityCube .\n",
      ".To achieve high quality  , these systems leverage a wide variety of data resources and KBC techniques.\n",
      ".A crucial challenge that these systems face is coping with imperfect or conflicting information from multiple sources .\n",
      ".To address this challenge  , we present an end-to-end KBC system called DeepDive.\n",
      ".1 DeepDive went live in January 2012 after processing the 500M English web pages in the ClueWeb09 corpus2   , and since then has been adding several million newly-crawled webpages every day.\n",
      ".Figure 1 shows several screenshots of DeepDive.\n",
      ".Similar to YAGO  and EntityCube   , DeepDive is based on the classic Entity-Relationship ER model  and employs popular techniques such as distant supervision  and the Markov logic language  to combine a variety of signals.\n",
      ".However  , DeepDive goes deeper in two ways: 1 Unlike prior large-scale KBC systems  , DeepDive performs deep natural language processing NLP to extract useful\n",
      ".Total of 3 filtered entities added\n",
      "Knowledge-base construction (KBC) is the process of populating a knowledge base (KB) with facts (or assertions) extracted from text.\n",
      "It has recently received tremendous interest from academia, e.g., CMU’s NELL [2] and MPI’s YAGO [7,9], and from industry, e.g., IBM’s DeepQA [5] and Microsoft’s EntityCube [16].\n",
      "To achieve high quality, these systems leverage a wide variety of data resources and KBC techniques.\n",
      "A crucial challenge that these systems face is coping with imperfect or conflicting information from multiple sources [3, 13].\n",
      "To address this challenge, we present an end-to-end KBC system called DeepDive.\n",
      "1 DeepDive went live in January 2012 after processing the 500M English web pages in the ClueWeb09 corpus2 , and since then has been adding several million newly-crawled webpages every day.\n",
      "Figure 1 shows several screenshots of DeepDive.\n",
      "Similar to YAGO [7,9] and EntityCube [16], DeepDive is based on the classic Entity-Relationship (ER) model [1] and employs popular techniques such as distant supervision [15] and the Markov logic language [11] to combine a variety of signals.\n",
      "However, DeepDive goes deeper in two ways: (1) Unlike prior large-scale KBC systems, DeepDive performs deep natural language processing (NLP) to extract useful\n",
      "\n",
      "Entities labelled ['ClueWeb09', 'EntityCube', 'DeepDive']\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "print('started extraction for the', model_name, 'model')\n",
    "path_to_model = ROOTPATH + '/crf_trained_files/' + model_name + '_TSE_model_0.ser.gz'\n",
    "\n",
    "# use the trained Stanford NER model to extract entities from the publications\n",
    "ner_tagger = StanfordNERTagger(path_to_model, STANFORD_NER_PATH)\n",
    "sentences = nltk.sent_tokenize(input_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub(r'\\[[^\\(]*?\\]', r'', sentence)\n",
    "    sentence = re.sub(r\"(\\.)([A-Z])\", r\"\\1 \\2\", sentence)\n",
    "    sentence = sentence.replace(\"@ BULLET\", \"\")\n",
    "    sentence = sentence.replace(\"@BULLET\", \"\")\n",
    "    sentence = sentence.replace(\", \", \" , \")\n",
    "    sentence = sentence.replace('(', '')\n",
    "    sentence = sentence.replace(')', '')\n",
    "    sentence = sentence.replace('[', '')\n",
    "    sentence = sentence.replace(']', '')\n",
    "    sentence = sentence.replace(',', ' ,')\n",
    "    sentence = sentence.replace('?', ' ?')\n",
    "    sentence = sentence.replace('..', '.')\n",
    "    print(sentence)\n",
    "    tagged = ner_tagger.tag(sentence.split())\n",
    "\n",
    "    for jj, (a, b) in enumerate(tagged):\n",
    "        tag = model_name.upper()\n",
    "        if b == tag:\n",
    "            a = a.translate(str.maketrans('', '', string.punctuation))\n",
    "            try:\n",
    "                if sentences[jj + 1][1] == tag:\n",
    "                    temp = sentences[jj + 1][0].translate(str.maketrans('', '', string.punctuation))\n",
    "                    bigram = a + ' ' + temp\n",
    "                    result.append(bigram)\n",
    "            except:\n",
    "                result.append(a)\n",
    "                continue\n",
    "            result.append(a)\n",
    "    print('.', end='')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "result = list(set(result))\n",
    "result = [w.replace('\"', '') for w in result]\n",
    "filtered_words = [word for word in set(result) if word not in stopwords.words('english')]\n",
    "filtered_words = [word for word in filtered_words if not wordnet.synsets(word)]\n",
    "print('Total of', len(filtered_words), 'filtered entities added')\n",
    "sys.stdout.flush()\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "print('')\n",
    "print('Entities labelled', filtered_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
