{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import ner_training, expansion, seed_data_extraction\n",
    "from postprocessing import training_data_generation, extract_new_entities, filtering\n",
    "import config as cfg\n",
    "import gensim\n",
    "import elasticsearch\n",
    "from config import ROOTPATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es = elasticsearch.Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "model_doc2vec = gensim.models.Doc2Vec.load(cfg.ROOTPATH + '/models/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seeds = ['clueweb', 'imagenet', 'flickr', 'webkb', 'netflix', 'imdb']\n",
    "context_words = ['dataset', 'corpus', 'collection', 'repository', 'benchmark', 'website']\n",
    "sentence_expansion = True\n",
    "training_cycles = 2\n",
    "model_name = 'dataset'\n",
    "training_cycle = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for cycle in range(training_cycles):\n",
    "    print(cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#  seed_data_extraction.sentence_extraction(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training data extraction\n",
      "Selected seed terms ['clueweb', 'webkb', 'imagenet', 'netflix', 'flickr', 'imdb']\n",
      "clueweb\n",
      "Got 393 hits in ES\n",
      "webkb\n",
      "Got 124 hits in ES\n",
      "imagenet\n",
      "Got 41 hits in ES\n",
      "netflix\n",
      "Got 548 hits in ES\n",
      "flickr\n",
      "Got 2521 hits in ES\n",
      "imdb\n",
      "Got 448 hits in ES\n",
      "2546 sentences added for training in iteration\n",
      "\n",
      "Process finished with 6 seeds\n"
     ]
    }
   ],
   "source": [
    "seed_data_extraction.sentence_extraction(model_name, seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#  expansion.term_expansion_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started to extract generic named entity from sentences...\n",
      "Finished processing sentences with 5259 new possible entities\n",
      "started clustering\n",
      "......added 1548 expanded terms\n",
      ".........................................."
     ]
    }
   ],
   "source": [
    "expansion.term_expansion(model_name, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding sentences\n",
      "Added 2525 expanded sentences to the 4101 original\n"
     ]
    }
   ],
   "source": [
    "expansion.sentence_expansion(model_name, 0, model_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.doc2vec.Doc2Vec"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainingdata_generation.generate_training_term_expansion_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling\n",
      "4095 lines labelled\n"
     ]
    }
   ],
   "source": [
    "trainingdata_generation.generate_training_term_expansion_only(model_name, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainingdata_generation.label_sentences_term_sentence_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling\n",
      "4101 lines labelled\n"
     ]
    }
   ],
   "source": [
    "trainingdata_generation.sentence_labelling(model_name, 0, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ner_training.create_austenprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner_training.create_prop(model_name, 0, sentence_expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner_training.train_model(model_name, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started extraction..... dataset cycle 0\n",
      "815\n",
      "...............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................797\n",
      ".............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................0\n",
      "0\n",
      "Total of 1751 filtered words added\n"
     ]
    }
   ],
   "source": [
    "extract_new_entities.ne_extraction(model_name, 0, sentence_expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering...\n",
      "Started  pmi filtering...\n",
      "Calculating npd with dataset\n",
      "!....!.....!....!.!.........!......!..!.....!....................................!...!...........!.....!........!....!......!...!.!................!.............................!.............!...!....!...!..!................!.....!..!...!........!......!..!.!.............!.....!..!............!..!......!...!.....!.....................!....!......!...........!.......!........!......!..............................!..!..!.....!.....!.....!..!..!...!..!..!.!....!.......!........!...!.......!.!.!.......!...!..!........!....!.......!.......!.!..!......!.....!...!.!.....!....!..!...............!........................!.....!.....!.........!.!..........!.....!........!.!....!...............!....!........!......!....!.!.!.!..!................!.!.....!.......!..............!................!................!..............!.!.!....!.....!.!.....!........!..!...!...!.!...........!......!..........!..!.....!....!.........!...........!..!........!.......!.!.....!....!..................!.........!..!.....!..........!.....!...!.!............!.........!.!....!.......!..............!..!.!...!...!..Calculating npd with corpus\n",
      "..!..!....!.!.!.!.!...!..!..........!.....!......!...!...........!.......!..!............!.............!....!..!.........!.....!.......!....!..!..........!..!.........!..........!.....!..!........!.......!...!....!.....!......!.!.....!.!.!...!..........!.....!..!.!.....!......!..!.................!.....!....!...!.!.!...........!....!.......!.....!..............!........!.............!........!...!.!........!..!.!...!.......!..!.........!....!......!....!..........!....!.....!....!.!...........!.....!..!....!.....!........!........!.................!..!.....!..!....................!........!...!.!...!..........!..!....!.....!...!.....!....!.!..!......!.!.!..........!...................................!.!.....!.....!.!......!..........!.........!..!.!......!......!...!....!..!...!.............!.....!...!..!.......................!...........!.....!...................!.......!.!.....!.!.........!.....!...!.....!...!....!..!....!.!......!..!.!....!.!.........!..!......!.....Calculating npd with collection\n",
      ".!.!...!.!.........!...!.!....!..............!.....!.!.......!........!........!.......!.......!..!....!...!..!...............!................!.....!.....!...!...!..!..!.....!....!....!.................!..!.............!...!..!...!.......!.!...........!.........................!.!.......!..........!.........!....!.......!........!......................!.............!...!....!....!...!....!......!..!.....!.......!.!.!...!....!.!......!.!....!...!.!.....................!.!............!..............!..............................!..............!.........................!....!............!....!...................!.....!.!.................!...!......!.!..............!....!..!.!...!..........!.............!.....................!.!..!..!..!......!...!.....!............!.........!.!..................!...!.........!.......!....!...!...!...........!.!......................!.!....!...!.............!...........!...!....!..........!.......!..!..........!...!.!...!.!.!..!......!.....!.....!...!......!.!...!..!..!....!.......................!.!.....!..............!........!....!...!..............!....!........!.....!.!........................!..!.!.!.......................!.!.!................!..!....!.............Calculating npd with repository\n",
      "!..!.!.....!..!.!.!........!....!....!....!.......!...!..!..!.!....................!......!......!.!.....!.!..!..!...!.....!.......!.!......!..!...!....!..!....!......!.!...!.!.....!.!..!.!..!....!.....!....!....!......!..!..!..!.!...!.......!...........!....!.....!.!.!.....!....!.......!....!.!.!........!.!..........!.........!.!.....!....!.!....!...!..!.!.!.!.!....!.!.!.......!......!..!..!..!.......!...!..!.....!....!....!........!.!....!....!....!.....!...........!........!.!....!..!.!.!.!......!..!..!..!....!..................!..!...!.!.....!.!..!..!.!..!.......!..!..........!....!...!..!..!....!.........!.!...!.!........!.........!.......!.....!.!.!...!.!...!.....!..!.....!.!.!..........!........!...!.!..!.....!..!..!....!.!.!.!..!..............!..!.!..!.!.....!...........!......!......!.!.........!.!.....!.....!..!....!.....!..............!..!....!....!...........!.!.....!..!....!...!....!.!........!.!..!.........!....!....!.....!....!.!..!...!.!...!........!...!....!...!.!.........Calculating npd with benchmark\n",
      "!..!.!...........!.......!.............!.............!..!......!....!.........!...!.....!...!.......................!........!.....!.....!.....!...!....!.....!.................!.....................!..!....!..!......!............!.......................!........!...........................!....!.....!....!................!.!.....!.................!................!.........!............!.....................................!.....!...........................!..........!..................!.............!.......!.......!.!.......!......!....!............!......................!......!...........................!..!....!..!.....!.................!.......!.!....!..........!.!..!.!..!............!..........Calculating npd with website\n",
      "!.!...!..!...!......!...!......!...!.!...!......!..!...............!...................!.....!...!......!.........!.....!.....!....!..!....!....!.....!..!.!..!....!......!..!..!..!.!.......!...!.!...!.......!...!.!......!.........!.........!....!.!.....!.!...!......!..!.................!...!...!......................!..!.!.......!...!......!......!.!.............!.!...!...!.....!......!....!...!................!.!.......!....!......!.....!.!.......!.!.!.......!.!.!.!..!.!........!.........!............!..!....!..!.!.....!...............!...!.......!.!.!..!..............!.....!.....!.......!..!....!..!.!......!......!...........!.!....!..!...!..!.......!..!..!...!..!.............!.....!.!.........!...............!.!...!.............!....!.!..!.!..!.!.....!.!.!......!...........!.!.!....!...!.!.......!.!.!.!..........!..!......!...!.....!...!.....!.!...!.!..!...........!.........!....!.!....!.....!..!....!...!...!......629 entities filtered from 1751\n"
     ]
    }
   ],
   "source": [
    "filtering.pmi(model_name, training_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy\n",
    "import string\n",
    "from numbers import Number\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from config import ROOTPATH\n",
    "from postprocessing import normalized_pub_distance\n",
    "\n",
    "stopword_path = ROOTPATH + \"/data/stopword_en.txt\"\n",
    "stopword_list = []\n",
    "with open(stopword_path, 'r') as file:\n",
    "        for sw in file.readlines():\n",
    "            stopword_list.append(sw.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 1854 with WordNet and Stopwords\n"
     ]
    }
   ],
   "source": [
    "extracted_entities = []\n",
    "path = ROOTPATH + '/processing_files/' + model_name + '_extracted_entities_' + str(training_cycle) + '.txt'\n",
    "with open(path, \"r\") as f:\n",
    "    for e in f.readlines():\n",
    "        extracted_entities.append(e.strip())\n",
    "print('Filtering', len(extracted_entities), 'with WordNet and Stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 1854 with WordNet and Stopwords\n"
     ]
    }
   ],
   "source": [
    "extracted_entities = []\n",
    "path = ROOTPATH + '/processing_files/' + model_name + '_extracted_entities_' + str(training_cycle) + '.txt'\n",
    "with open(path, \"r\") as f:\n",
    "    extracted_entities = [e.strip() for e in f.readlines()]\n",
    "print('Filtering', len(extracted_entities), 'with WordNet and Stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846\n",
      "1801\n"
     ]
    }
   ],
   "source": [
    "ws_filtered = []\n",
    "stopword_filtered = [word for word in set(extracted_entities) if word.lower() not in stopwords.words('english')]\n",
    "print(len(stopword_filtered))\n",
    "stopword_filtered = [word for word in set(stopword_filtered) if word.lower() not in stopword_list]\n",
    "print(len(stopword_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_by_wordnet = [word for word in stopword_filtered if not wordnet.synsets(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_by_wordnet = []\n",
    "for word in set(stopword_filtered):\n",
    "    if not wordnet.synsets(word):\n",
    "        filter_by_wordnet.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055\n"
     ]
    }
   ],
   "source": [
    "print(len(filter_by_wordnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055\n"
     ]
    }
   ],
   "source": [
    "print(len(filter_by_wordnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = ROOTPATH + '/processing_files/' + model_name + '_extracted_entities_' + str(training_cycle) + '.txt'\n",
    "with open(path, \"r\") as f:\n",
    "    extracted_entities = [e.strip().lower() for e in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 1854 entities with embedded clustering\n",
      "........716 entities are kept from the total of 1854\n"
     ]
    }
   ],
   "source": [
    "filtering.filter_st_pmi_kbl_ec(model_name, training_cycle, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 1854 entities with PMI\n",
      "678 entities are kept from the total of 1854\n"
     ]
    }
   ],
   "source": [
    "filtering.filter_pmi(model_name, training_cycle, context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 1854 entities by vote of selected filter methods\n",
      "598 entities are kept from the total of 1854\n"
     ]
    }
   ],
   "source": [
    "filtering.majority_vote(model_name, training_cycle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
