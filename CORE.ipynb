{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root = 'sci_paper_miner/'#Path(__file__).parent.absolute()\n",
    "data_root = root / Path(\"data\")\n",
    "\n",
    "core_dataset_name = data_root / Path('arxiv_2018')\n",
    "core_cs_query_path = core_dataset_name / Path('raw_query')\n",
    "core_cs_db_path = core_dataset_name / Path('db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_output_dir = core_cs_query_path\n",
    "for lck_file in q_output_dir.glob('*.lck'):\n",
    "    print(lck_file)\n",
    "#     os.remove(str(lck_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fulltext_0.json\n",
      "fulltext_1.json\n",
      "fulltext_2.json\n",
      "fulltext_3.json\n"
     ]
    }
   ],
   "source": [
    "for root, dirnames, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        if filename.startswith('full'):\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fulltext_0.json',\n",
       " 'fulltext_1.json',\n",
       " 'fulltext_2.json',\n",
       " 'metadata.json',\n",
       " 'fulltext_3.json']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9071 with full text 28255 without\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "path = 'sci_paper_miner/data/arxiv_2018/db/'\n",
    "for x in range(4):\n",
    "    filename = 'fulltext_'+ str(x) +'.json'\n",
    "    json_file = os.path.join(path, filename)\n",
    "    for line in open(json_file):\n",
    "        articles.append(json.loads(line))\n",
    "\n",
    "titles = []\n",
    "ids = []\n",
    "articles_full = {}\n",
    "x, y = 0, 0\n",
    "for article in articles:\n",
    "    titles.append(article['title'])\n",
    "    if article['fullText']:\n",
    "        ids.append(article['id'])\n",
    "        articles_full[article['id']] = article\n",
    "        x+=1\n",
    "    else:\n",
    "        y+=1\n",
    "print(x, 'with full text', y, 'without')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3346"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authors': ['Siméoni, Oriane',\n",
       "  'Iscen, Ahmet',\n",
       "  'Tolias, Giorgos',\n",
       "  'Avrithis, Yannis',\n",
       "  'Chum, Ondrej'],\n",
       " 'contributors': [],\n",
       " 'datePublished': '2018-01-24',\n",
       " 'description': 'Severe background clutter is challenging in many computer vision tasks,\\nincluding large-scale image retrieval. Global descriptors, that are popular due\\nto their memory and search efficiency, are especially prone to corruption by\\nsuch a clutter. Eliminating the impact of the clutter on the image descriptor\\nincreases the chance of retrieving relevant images and prevents topic drift due\\nto actually retrieving the clutter in the case of query expansion. In this\\nwork, we propose a novel salient region detection method. It captures, in an\\nunsupervised manner, patterns that are both discriminative and common in the\\ndataset. Saliency is based on a centrality measure of a nearest neighbor graph\\nconstructed from regional CNN representations of dataset images. The\\ndescriptors derived from the salient regions improve particular object\\nretrieval, most noticeably in a large collections containing small objects',\n",
       " 'doi': None,\n",
       " 'downloadUrl': '',\n",
       " 'ft_file_num': 2,\n",
       " 'ft_line_num': 4524,\n",
       " 'fullText': 'Unsupervised deep object discovery for instance recognition\\nOriane Sime´oni1 Ahmet Iscen1 Giorgos Tolias2 Yannis Avrithis1 Ondrˇej Chum2\\n1Inria Rennes 2VRG, FEE, CTU in Prague\\n{oriane.simeoni,ahmet.iscen,ioannis.avrithis}@inria.fr\\n{giorgos.tolias,chum}@cmp.felk.cvut.cz\\nAbstract\\nSevere background clutter is challenging in many com-\\nputer vision tasks, including large-scale image retrieval.\\nGlobal descriptors, that are popular due to their memory\\nand search efficiency, are especially prone to corruption\\nby such clutter. Eliminating the impact of the clutter on\\nthe image descriptor increases the chance of retrieving rel-\\nevant images as well as preventing topic drift by actually\\nretrieving the clutter in the case of query expansion. In this\\nwork, we propose a novel salient region detection method.\\nIt captures, in an unsupervised manner, patterns that are\\nboth discriminative and common in the dataset. The de-\\nscriptors derived on the salient regions improve particular\\nobject retrieval, most noticeably in a large collections con-\\ntaining small objects.\\n1. Introduction\\nParticular object retrieval becomes very challenging\\nwhen the object of interest is covering a small part of the\\nimage. In this case, the amount of relevant information is\\nsignificantly reduced. Large objects might be partially oc-\\ncluded, while small objects are on a background that covers\\nmost of the image. A combination of both, occlusion and\\ncluttered background, is not rare either. These conditions\\nnaturally arise from image acquisition and make naive ap-\\nproaches fail, including global template matching or semi-\\nrobust template matching [25].\\nIdeally, image descriptors should be extracted only from\\nthe relevant part of the image, suppressing the irrelevant\\nclutter and occlusions. In this paper, we attempt to de-\\ntermine the regions containing the relevant information, as\\nshown in Figure 1, in a fully unsupervised manner.\\nMethods based on robust matching of hand-crafted lo-\\ncal features are naturally insensitive to occlusion and back-\\nground clutter. The locality of the features allows to match\\nsmall parts of images in regions containing the object of in-\\nterest, while the incorrect matches are typically removed by\\nrobust geometric consistency check [28]. Methods based\\nFigure 1. The saliency map (right) computed for an input image\\n(left) based on common-structure analysis on Instre dataset. Back-\\nground clutter and objects not relevant for this dataset are auto-\\nmatically removed. The image is represented only by the region\\ndetected on the saliency map.\\non efficient matching of vector-quantized local-feature de-\\nscriptors were introduced in context of image retrieval by\\nSivic and Zisserman [35].\\nRetrieval methods based on descriptors extracted by con-\\nvolutional neural networks (CNNs) have become popular\\nbecause they combine good precision and recall, efficiency\\nof the search, and reasonable memory footprint [5, 30].\\nDeep neural networks are capable of learning, to some\\nextent, what information in the image is relevant, which\\nresults in a good performance even with global descrip-\\ntors [39, 4, 17]. However, if the signal to noise ratio is\\nlow, e.g. the object is relatively small, multiple objects are\\npresent, etc., the global CNN descriptors fail [12, 11].\\nA class of methods inspired by object detection have re-\\ncently emerged. Instead of attempting to match the whole\\nimage to the query, the problem is changed to finding\\na rectangular region in the image that best matches the\\nquery [39, 31]. An inefficient search by sliding window is\\nintractable for large collections of images. The exhaustive\\nenumeration is approximated by similarity evaluation on a\\nnumber of pre-selected regions. The regions are either se-\\nlected geometrically to cover the whole image at different\\nscales, as in R-MAC [39], or by considering the content by\\nobject or region proposal methods [31, 36, 8].\\nAnother direction of suppressing irrelevant content is\\nsaliency detection [17, 24]. For each image, a saliency map,\\n1\\nar\\nX\\niv\\n:1\\n70\\n9.\\n04\\n72\\n5v\\n1 \\n [c\\ns.C\\nV]\\n  1\\n4 S\\nep\\n 20\\n17\\nthat captures more general region shapes compared to (a\\nsmall set of) rectangles, is first estimated. The contribution\\nof each pixel (or region) is then proportional to the saliency\\nof that location.\\nIn this work we introduce a very simple pooling scheme\\nthat inherits the properties of both saliency detection and\\nregion based pooling and that, like all previous approaches,\\nis applied to each image in the database independently. In\\naddition, we investigate the use of the resulting regional rep-\\nresentation for automatic, offline object discovery and sup-\\npression of background clutter, which considers the image\\ncollection as a whole. Unlike previous approaches, we do\\nthis in an unsupervised way. As a consequence, our rep-\\nresentation takes two saliency detection steps into account.\\nOne that acts per image and depends solely on its content\\nand another that considers the image collection as a whole\\nand captures frequently appearing objects.\\nIn both cases, we derive a global representation that out-\\nperforms comparable state-of-the-art methods in retrieving\\nsmall objects on standard benchmarks, while the memory\\nfootprint and online cost is only a fraction compared to\\nmore powerful regional representations [30, 12]. Moreover,\\nwe show that our representation benefits significantly from\\nquery expansion methods.\\nSection 2 discusses our contributions against related\\nwork. Section 3 describes our methodology including our\\npooling scheme in Section 3.3 and our object discovery ap-\\nproach in Section 3.8. We present experimental results in\\nSection 4 and draw conclusions in Section 5.\\n2. Related work\\nLocal features and geometric matching offer an attrac-\\ntive way for retrieval systems to handle occlusions, clutter,\\nand small objects [35, 28, 13]. One of their drawbacks is\\nhigh query complexity and large storage cost; an image is\\ntypically represented by several thousands features. Many\\nmethods attempt to decrease the amount of indexed features\\nby removing background clutter while maintaining the rele-\\nvant information. The selection procedure is either applied\\nindependently per image or considers an image collection as\\na whole. Common examples of the former case are bursty\\nfeature detection [33], symmetry detection [38] or use of\\nsemantic segmentation [1, 26]. The methods of the second\\ncategory, are scalable enough to jointly process the whole\\ncollection and perform feature selection by the following\\nassumption. A feature that repeats over multiple instances\\nof the same object in the dataset is likely to appear in novel\\nviews of the object too. Representative cases are common\\nobject discovery [40, 37], co-occurrence detection [6], or\\nmethods using GPS information [7, 19].\\nThe work by Turcot and Lowe [40] performs pairwise\\nspatial verification on hand-crafted local features across all\\nimages and only indexes verified features. With an addi-\\ntional off-line cost, the on-line stage is sped up and the\\nmemory footprint is reduced. However, unique views of\\nobjects are not verified and thus discarded. In this work, we\\naddress a similar selection problem based on more powerful\\nCNN-based representation rather than local features.\\nRecent advances on deep learning [3, 39, 17, 9, 29] dis-\\npense with the large memory footprint by using global de-\\nscriptors and cast the problem of instance search as Eu-\\nclidean nearest neighbor search. Nevertheless, background\\nclutter and occlusion are better handled by regional repre-\\nsentation. Regional descriptors significantly increase the\\nperformance when they are indexed independently [30, 12]\\nbut this comes at a prohibited memory and computational\\ncost for large scale scenarios. Region Proposal Networks\\n(RPN) are applied either off-the-shelf [31] or after fine-\\ntuning [36] for instance search. The RPNs reduce the num-\\nber of regions per image only to the order of tens. Our work\\nfocuses on aggregating regional representation that keeps\\nthe complexity low but we rather detect regions around\\nsalient objects and objects that frequently appear in the\\ndataset. Jimenez et al. [15] construct saliency maps and\\nperform region detection to construct global image vectors,\\nas we also do. However, they employ generic object de-\\ntectors trained on ImageNet and this makes the method not\\napplicable with fine-tuned networks which provide the best\\nperformance. The Hessian-affine detector is used on CNN\\nactivations to detect repeatable regions [14]. The major ben-\\nefit in this work, though, comes from second order pooling\\nand higher dimensional descriptors.\\nSaliency maps are another way to handle clutter and oc-\\nclusions. Once more, there exist both examples of computa-\\ntion in an unsupervised manner [17, 20] or learned [24, 16]\\nand applied per image afterwards. Our approach generates\\nsaliency maps in a fully unsupervised way that capture both\\nsalient objects on single images but also repeating objects\\nappearing in a particular image collection.\\n3. Method\\nLike [40], our objective is to remove transient and non-\\ndistinctive objects as in Figure 1 and rather focus on objects\\nappearing frequently in a dataset. Beginning with the acti-\\nvation map of a convolutional layer in a CNN, one would\\nneed access to a local representation to automatically dis-\\ncover such objects. On the other hand, knowing what these\\nobjects are would help forming a local representation by\\nselecting regions depicting them, which appears to be a\\nchicken-and-egg problem. Without an initial region selec-\\ntion, we risk “discovering” uninformative but frequently ap-\\npearing “stuff”-like patches, for instance sky.\\n3.1. Overview\\nFortunately, it is possible to make an initial selection\\nbased on CNN activations alone, without any training and\\ndataset feature saliency FS regions\\nregion graph object saliency OS regions\\nFigure 2. Overview of our offline unsupervised process. On the\\ntop row, CNN activations of dataset images are used to extract a\\nfeature saliency map, on which a set of regions is detected. On\\nthe bottom row, a centrality measure is obtained per region from\\na region k-NN graph. Using this measure, a dense object saliency\\nmap is formed from the original CNN activations and the feature\\nsaliency. This map is focusing on objects automatically discovered\\nin the dataset, with background clutter removed. Finally, another\\nset of regions is detected on the object saliency map to extract\\ndescriptors and represent the dataset for retrieval.\\nwithout bounding box annotations. As described in Sec-\\ntion 3.3, the mechanism is inspired by CroW [17] and Grad-\\nCAM [32] and generates a feature saliency map. This initi-\\nates our offline analysis illustrated in Figure 2. A small set\\nof rectangular regions is detected per image from this map\\nas discussed in Section 3.4. This first round of detection\\nis applied independently per image and depends only on its\\ncontent.\\nEach region in the dataset is associated to a feature\\nsaliency score and a visual descriptor, pooled from the ac-\\ntivation map of the corresponding image, as discussed in\\nSection 3.5. It is now possible to compute a centrality score\\nper region, representing the “significance” of each region in\\nthe dataset. This is based on a region k-NN graph and is\\ndiscussed in Sections 3.6 and 3.7.\\nNow, given a new image, we can infer the “significance”\\nof every region from its nearest neighbors in the graph,\\nyielding a dense object saliency map as discussed in Sec-\\ntion 3.8. This is a regression problem and we suggest a\\nnon-parametric k-NN solution. Finally, we detect a small\\nset of rectangular regions on this saliency map and extract a\\nglobal descriptor to represent dataset images for retrieval, as\\ndiscussed in Section 3.9. This second detection procedure\\ntakes into account all salient and repeating objects appear-\\ning in the dataset.\\nThe entire process is fully unsupervised and only as-\\nsumes on the-shelf networks trained on a classification or\\nretrieval task without bounding box annotations.\\n3.2. Representation\\nWe represent the activation map of a convolutional layer\\nas a non-negative 3d tensor A ∈ Rh×w×c where h,w are\\nthe spatial resolution (height, width) and c is the number of\\nfeature channels. The set of valid spatial positions is P : =\\n[h] × [w]1 and the set of all rectangles with vertices in P\\nis denoted by R. By Apj we represent an element of A at\\nposition p ∈ P and channel j ∈ [c]. By A•j ∈ Rh×w we\\ndenote the 2d feature map of A corresponding to channel\\nj ∈ [c]. By Ap• ∈ Rc we denote the vector containing all\\nfeature channels at position p ∈ P .\\n3.3. Feature saliency\\nInspired by cross-dimensional weighting and pooling\\n(CroW) [17] and class activation mapping (CAM) [44], we\\nconstruct a 2d saliency map of an image based on a con-\\nvolution activation of that image alone. Following CroW,\\nwe compute an idf-like weight per channel b ∈ Rc with\\nelements\\nbj = log\\n(\\n(a+ \\x0f)>1\\naj + \\x0f\\n)\\n(1)\\nfor j ∈ [c], where a : = 1wh\\n∑\\np∈P 1[Ap•] ∈ Rc is the\\naverage number of nonzero elements per channel. We then\\ncompute a weighted sum over channels\\nF : =\\n∑\\nj∈[c]\\nbjA•j (2)\\nFinally, we obtain the 2d feature saliency (FS) map Fˆ ∈\\nRh×w by normalizing F according to [17]. Contrary to\\nCroW, we use the feature channel weights when comput-\\ning the 2d spatial weights, amplifying channels with sparse\\nactivation. This order of summation is the same as in CAM.\\nHowever, we are working with channel weights obtained by\\na sparsity property on any convolutional layer, without any\\nassumption on the network topology. CAM on the other\\nhand, assumes global average pooling followed by a fully\\nconnected layer mapping channels to classes and uses the\\nparameters of this layer to obtain a saliency map per class.\\n3.4. Region detection\\nWe are given a 2d saliency map S, which can be either\\nthe feature saliency described in section 3.3 or the object\\nsaliency described in Section 3.8. We use an expanding\\nGaussian mixture (EGM) model [2] to detect a number of\\nsalient rectangular regions. This is a variant of expectation-\\nmaximization (EM) that iteratively performs local averag-\\ning (E- and M-steps) interleaved with a selection process\\n(P-step) similar to non-maximum suppression (NMS). In\\ndoing so, it dynamically estimates the number of regions.\\n1Here, [i] is the set {1, . . . , i} for i ∈ N.\\nimage i = 0,m = 272 i = 2,m = 29 i = 3,m = 22 i = 5,m = 17 i = 7,m = 11 i = 14,m = 9\\nFigure 3. Evolution of regions during EGM iterations on the feature saliency map of an image of Magdalen tower from Oxford buildings\\ndataset, shown on the left. Below each image we display the iteration i and the number of regions m.\\nThe original algorithm applies to point sets and isotropic\\nGaussian components. Here we extend it to functions, con-\\nsidering that a saliency map is a function S : P → R. We\\nuse it to fit a number of components, each modeling a rect-\\nangular region in 2d coordinate space. We also extend it to\\na diagonal covariance model, so that a rectangle is modeled\\nby an axis-aligned ellipse.\\nIn particular, given 2d saliency map S ∈ Rh×w, we rep-\\nresent it as a set of Gaussian functions si : R2 → R with\\nsi(x) : = SpiN (x|pi, σI2) (3)\\nfor i ∈ [`], x ∈ R2 where N is the normal density,\\n` = |P | is the number of positions and we represent P as\\n{p1, . . . , p`}. Here, σ is a scale parameter that determines\\nhow coarse or fine the region representation will be for the\\ngiven saliency map. Similarly, we represent components as\\nGaussian functions qk : R2 → R with\\nqk(x) : = pikN (x|µk,Σk) (4)\\nfor k ∈ [m], x ∈ R2, wherem is the number of components\\nand pik ∈ R, µk ∈ R2 and Σk ∈ R2×2 are the mixing coef-\\nficient, mean and diagonal covariance matrix respectively\\nof component k. Means represent region centers, while\\nthe (inverse) eigenvalues of covariance matrices represent\\nheights and widths. We initialize components as qk ← sk\\nfor k ∈ [m], with m ← `. In the expectation (E)-step, we\\ncompute the responsibility\\nγik ← 〈si, qk〉∑\\nj∈[m] 〈si, qj〉\\n(5)\\nof component k ∈ [m] for sample i ∈ [`], where 〈f, g〉 is the\\nL2 inner product of square-integrable functions f, g : Rd →\\nR, computed in closed form for Gaussian functions [2]. In\\nthe maximization (M)-step, we update parameters as\\npik ← `k\\n`\\n(6)\\nµk ← 1\\n`k\\nn∑\\ni=1\\nγikpi (7)\\nΣk ← 1\\n`k\\nn∑\\ni=1\\nγik diag(pi − µk)◦2 (8)\\nwhere `k : =\\n∑n\\ni=1 γik is the effective number of points as-\\nsigned to component k andX◦2 : = X ◦X is the Hadamard\\nproduct power for a vector or matrix X .\\nFinally, in the purge (P)-step, similarly to NMS, we pro-\\ncess components in descending order of mixing coefficient\\nand we decide whether to keep a component or not depend-\\ning on its overlap with the collection of previously kept\\ncomponents. Overlap is measured by a generalized respon-\\nsibility function similar to (5), and again inner products are\\ngiven in closed form [2]. This means that the number of\\ncomponents m is potentially reducing at each iteration.\\nFigure 3 shows how regions are formed during EGM it-\\nerations, starting from one small region centered on each\\nspatial position. We get 4 clean regions on the ground truth\\nbuilding, as well as 6 regions on background objects, which,\\nalthough less salient, cannot be removed based on the fea-\\nture saliency alone.\\n3.5. Region pooling\\nGiven a rectangular region R ∈ R of an image with fea-\\nture saliency map Fˆ ∈ Rh×w, we associate to it feature\\nsaliency f : = µFˆ (R) ∈ R, where\\nµFˆ (R) : =\\n1\\n|R|\\n∑\\np∈R\\nFˆp (9)\\nis the average of 2d map Fˆ over R.\\nIn addition, given the activation map A ∈ Rh×w×c of\\nthe same image, it is standard practice that a descriptor is\\nobtained by pooling over R, for instance sum [4], weighted\\nsum [17] or max [3, 39] pooling. We adopt the latter choice\\nto extract descriptor z : = mA(R) ∈ Rc, where\\nmA(R) : = max\\nq∈R\\nAq• (10)\\nis the maximum of 3d tensor A over R along the spatial\\ndimensions. This has been the basis of fine-tuning in [29, 8].\\nA particular set of regions, uniformly sampled on a grid\\nat different scales, is referred to as regional maximum acti-\\nvation of convolutions (R-MAC) [39]. Global description,\\nreferred to as MAC, is a special case where there is a single\\nregion R = P . In contrast, we detect a set of regions based\\non saliency maps in this work.\\nFinally, we follow [29] in performing supervised whiten-\\ning of the descriptors by simultaneous diagonalization [22].\\nIn particular, given vector z ∈ Rc, we `2-normalize, cen-\\nter, whiten, PCA-project and renormalize to generate the\\nregion descriptor v : = w(z) ∈ Rd for region R. Function\\nw : Rc → Rd represents the entire whitening and normal-\\nization pipeline.\\n3.6. Graph construction\\nGiven an image dataset, we assume here a set of re-\\ngions {R1, . . . , Rn} are detected from the saliency maps\\nas discussed in Section 3.4, a feature saliency vector f : =\\n(f1, . . . , fn) ∈ Rn is computed with the corresponding av-\\nerage saliency per region in (9), and a set of descriptors\\nV : = {v1, . . . ,vn} ⊂ Rd are extracted from the activation\\nmaps, whitened and normalized per region as discussed in\\nSection 3.5.\\nBased on the above information, we construct a k-NN\\ngraph on those regions in order to compute a global cen-\\ntrality score per region as discussed in Section 3.7, which\\nenables us to form an object saliency map on a new image,\\ndescribed in Section 3.8.\\nWe construct a weighted undirected graph having the\\nset of descriptors V as vertices. Following [12], the edge\\nweights are defined according to mutual k-nearest neigh-\\nbors (NN) in the descriptor space. In particular, given\\ndescriptors v,u ∈ Rd, we measure their similarity by\\ns(v,u) = (v>u)β , where exponent β > 0 is a parame-\\nter. We define the sparse symmetric nonnegative adjacency\\nmatrix W ∈ Rn×n with elements wij being s(vi,vj) if\\nvi,vj are mutual k-NN in V and zero otherwise.\\nWe define the n × n degree matrix D : = diag(W1)\\nwhere 1 ∈ Rn is the all-ones vector, and the symmetrically\\nnormalized adjacency matrix\\nW : = D−1/2WD−1/2, (11)\\nwith the convention 0/0 = 0. Following [12, 11], we define\\nthe n× n matrices Lα : = (D − αW )/(1− α) and\\nLα : = D−1/2LαD−1/2 = (I − αW)/(1− α), (12)\\nwhere α ∈ [0, 1). Both are positive-definite [12, 11].\\nGraph W\\nFˆ\\nS\\nFigure 4. Computing the object saliency map S of an image from\\nInstre dataset (top), as defined in (14). For each patch, its neigh-\\nbors in the graph (right) are found. Common patterns with high\\ncentrality in green outline, outliers with low centrality in red. S\\n(bottom) then focuses on patches similar to common patterns and\\ncombines with feature saliency Fˆ (left).\\n3.7. Graph centrality\\nWith the above definitions in place, the objective is to\\ncompute a vector g ∈ Rn where each element gi represents\\nthe significance of vertex vi in the graph, for i ∈ [n]. We\\ndefine this centrality vector as the solution g∗ ∈ Rn of the\\nlinear system\\nLαg = 1. (13)\\nAs in [12], we solve this system by the conjugate gradients\\n(CG) [23] method. Any method would be equally appropri-\\nate because this is computed just once offline.\\nThe solution g∗ is a graph centrality measure [21], and\\nin particular, Katz centrality [18]. Centrality is a global\\nmeasure of significance of vertices in a graph, and PageR-\\nank [27] is maybe the most well-known. In fact, Katz cen-\\ntrality was introduced as such a global measure before be-\\ning adapted by boundary condition y to measure relevance\\nto individual vertices by Hubbell [10]. This work has a long\\nhistory before being rediscovered e.g. by [27, 45], as sum-\\nmarized in the study of spectral ranking [41].\\n3.8. Saliency map construction\\nGiven the region descriptor set V , the region saliency\\nvector f and the associated centrality vector g∗ of an en-\\ntire dataset, the problem is to construct a new saliency map\\nS ∈ Rh×w for an image in the dataset. The image is rep-\\nresented by its activation map A ∈ Rh×w×c. Since this\\nsaliency is based on regions or patterns appearing frequently\\nin the dataset, which are commonly associated to repeating\\nobjects, we call it object saliency (OS).\\nWe compute S by a sliding window iteration over each\\nposition p ∈ P . The saliency value Sp at p is found as\\na linear combination of the centrality values of the nearest\\nneighbors in V of a patch centered at p. In particular, we\\nconsider a square patchRp of side a centered at p. We com-\\npute the vector up : = w(mA(Rp)) ∈ Rd by max-pooling\\nover Rp, whitening and normalizing as discussed in Sec-\\ntion 3.5. If Np is the set of indices of the k-NN of up in V ,\\nwe compute Sp as\\nSp : = Fˆ\\nΘ\\np\\n∑\\ni∈Np\\ns(vi,up)f\\nθ\\ni g\\n∗\\ni . (14)\\nThat is, each neighboring region descriptor vi is weighted\\nby its similarity to patch descriptor up, its feature saliency\\nfi and its centrality g∗i , while the entire sum is scaled by\\nthe feature saliency Fˆp at the current position p of the im-\\nage being considered. Exponents Θ and θ control the rela-\\ntive importance of feature saliency of the current image and\\nneighbors, respectively, compared to centrality. The object\\nsaliency computation is illustrated in Figure 4. Looking at\\nthe input image and is feature saliency map Fˆ alone, it is\\nnot evident which is the object of interest and which is clut-\\nter. This is only found by discovering other instances of the\\nsame object in the dataset, as represented by the graph.\\n3.9. Representation\\nThe object saliency map S highlights patterns that ap-\\npear frequently in the dataset, with the background clutter\\nremoved. It is only natural then to apply the same method\\ndescribed in Section 3.4 to this map in order to detect a\\nsmall number of regions per image. Unlike the regions de-\\ntected from the feature saliency map Fˆ , these new regions\\nare more likely to appear in a new image. For the purpose\\nof evaluation, we investigate both saliency maps.\\nFor each region R detected from a saliency map (Fˆ\\nor S) in a dataset image with activation map A, we ap-\\nply max pooling and `2-normalization. All descriptors are\\nthen summed and the resulting descriptor is whitened with\\nw : Rc → Rd as described in Section 3.5. The difference\\nhere is that we apply whitening on the aggregated vector\\nand not separately per region. This is the same representa-\\ntion as R-MAC evaluated in [29] and both yield a global im-\\nage representation in Rd, but here the regions are detected\\nin the saliency map rather than being uniformly distributed.\\nPooling based on saliency is in fact the idea explored\\nin CroW [17], but here we follow the nonlinear two-level\\npooling of R-MAC (max followed by sum) rather than the\\none-level sum of CroW. This is more powerful and has also\\nbeen the basis of fine-tuning in [8].\\n4. Experiments\\nWe apply the proposed representation on image retrieval.\\nIn particular, we have two variants of our method that both\\nuse the region detection described in Section 3.4. The\\nsaliency map which the detection is performed on is differ-\\nent in each case. FS.EGM uses the feature saliency map de-\\nscribed in Section 3.3, and OS.EGM uses the object saliency\\nmap described in Section 3.4. The former is image specific,\\nwhile the latter both image and database specific.\\n4.1. Experimental setup\\nTest sets. We evaluate on Oxford Buildings [28] and the\\nmore recently introduced Instre [42] dataset. Instre con-\\ntains around 27k images of small objects in cluttered scenes\\nwhile objects appear with different variations, such as rota-\\ntion, occlusion and scale changes, making it a challenging\\ncase. We use the evaluation protocol introduced in [12] for\\nInstre. We add 100k distractors from Flickr [28] to Ox-\\nford5k to perform experiments at larger scale. We refer to it\\nas Oxford105k. Search performance in all datasets is mea-\\nsured with mAP.\\nImage Representation. We represent each image by global\\nimage representation as described in Section 3.9. This re-\\nduces image similarity to cosine, which is common prac-\\ntice [39]. Feature extraction is performed with the VGG\\nnetwork [34] that is fine-tuned specifically for image re-\\ntrieval [29]. Supervised whitening [29] is used for post-\\nprocessing. The same network is additionally used to com-\\npare against two baselines. First, MAC global descriptor,\\nwhich is obtained by global max pooling and the descriptor\\nthat the network is directly optimized for [29]. Second, the\\nbaseline approach (Uniform), which refers to regional max\\npooling for regions that are uniformly sampled at 3 scales,\\nas in R-MAC [39]. Our variants are different in that regions\\nare detected from salient and repeating objects, while ag-\\ngregation and whitening is identical. Detection is applied\\nto dataset images only, while we use the provided bounding\\nboxes on the query side.\\nImplementation Details. To simplify region detection,\\neach saliency map is masked above threshold τ and\\nelement-wise raised to exponent ρ before detection, which\\nremoves the weakest regions and increases the contrast be-\\ntween foreground and background objects. We set ρ = 1,\\nτ = 0.2 and scale parameter σ = 1 before any parame-\\nter tuning is performed. We determine OS parameters Θ, θ\\nin (14) by visual inspection of OS and set Θ = 2, θ = 3\\nthroughout our experiments.\\n4.2. Parameter tuning\\nIn this section, we show the impact of FS.EGM and\\nOS.EGM detection parameters on the retrieval performance.\\nWe tune the parameters on Oxford5k when using diffu-\\nsion [12]. The remaining experiments evaluate the proposed\\nrepresentation with the chosen parameters on Instre and Ox-\\nford105k as well.\\nFeature saliency detection is evaluated first by FS.EGM,\\nwhile we do not compute object saliency and OS.EGM yet.\\nFigure 5 shows the effect of ρ, which controls the contrast\\nof the the saliency map. We observe that large ρ is needed\\nto remove as much clutter as possible from the noisy FS\\nactivations. We set ρ = 5 for the rest of our experiments.\\n1 2 3 4 5 6\\n84\\n86\\n88\\n90\\nExponent ρ\\nm\\nA\\nP\\nOS.EGM\\nFS.EGM\\nFigure 5. mAP on Oxford5k versus saliency exponent ρ for\\nFS.EGM and OS.EGM.\\n0.1 0.2 0.3 0.4 0.5 0.6\\n86\\n88\\n90\\nThreshold τ\\nm\\nA\\nP\\nFS.EGM\\nOS.EGM\\nFigure 6. mAP on Oxford5k versus threshold τ for FS.EGM and\\nOS.EGM.\\n1 1.5 2 2.5 3 4 5\\n88\\n89\\n90\\nScale σ\\nm\\nA\\nP\\nOS.EGM\\nFS.EGM\\nFigure 7. mAP on Oxford5k versus EGM scale parameter σ for\\nFS.EGM and OS.EGM.\\nFigure 6 shows the effect of threshold τ , which is another\\nselectivity parameter. We set τ = 0.4. Scale σ is used dur-\\ning EGM sampling as explained in Section 3.4. Its impact\\nin performance is shown in Figure 7. Setting σ = 2.5 re-\\nsults in good performance and regions that are large enough\\nfor FS.EGM.\\nObject saliency detection is then evaluated once the fea-\\nture saliency parameters are fixed, and EGM detection is\\napplied on the new saliency map. We observe that OS be-\\nhaves quite differently to FS, because foreground objects\\nare much cleaner. The impact of parameters σ and ρ is\\nshown in Figures 5 and 7 respectively. It is remarkable that\\na much lower exponent is needed in this case. We choose\\nρ = 2 and σ = 2. Finally, we fix τ = 0 for OS, as the\\nsaliency maps obtained with OS are exactly zero at back-\\nground regions. The effect is shown in Figure 6.\\n4.3. Evaluation of saliency maps\\nWe exploit the fact that Instre dataset comes with bound-\\ning box annotation for all database images. We use\\nthe ground truth information to quantitatively evaluate the\\nsaliency maps. We define precision as the sum of saliency\\nover ground truth regions, normalized by the sum over the\\nentire image, and we measure it for FS and OS as shown\\nin Figure 8. High precision means that a saliency map is\\n0 0.2 0.4 0.6 0.8 1\\n0.05\\n0.1\\n0.15\\nSaliency precision\\nFigure 8. Histogram of saliency precision for FS and OS maps\\nmeasured on all images of Instre.\\nimage FS.EGM OS.EGM\\nFigure 9. Examples of images from Oxford5k (first 2 rows) and\\nInstre (last 3 rows) datasets, along with smoothed FS and OS maps\\nsuperimposed on the images and regions detected by EGM, in red.\\nwell aligned to the ground truth bounding boxes. Given\\nthat these bounding boxes are not used anywhere, the im-\\nprovement that OS offers is impressive. Visual examples\\nfor saliency maps and detections for FS.EGM and OS.EGM\\nare shown in Figure 9. In all cases, OS is cleaner and fo-\\ncuses on objects that FS cannot discriminate.\\n4.4. Comparison to other methods\\nWe compare our methods to the standard practice of uni-\\nform region sampling (Uniform) as in R-MAC and global\\nmax pooling (MAC). We additionally propose a variant of\\nMethod QE Instre Oxford Oxford105k\\nMAC - 48.5 79.7 73.9\\nUniform [39] - 47.7 77.7 70.1\\nFS.EGM ? - 48.4 77.5 70.2\\nOS.EGM ? - 50.1 79.6 71.8\\nOS.EGM-4? - 53.7 79.8 71.4\\nMAC X 71.8 87.4 86.0\\nUniform [39] X 70.3 85.7 82.7\\nFS.EGM ? X 71.2 89.8 87.9\\nOS.EGM ? X 72.7 90.4 88.0\\nOS.EGM-4? X 75.4 90.1 84.3\\nTable 1. mAP comparison of our methods marked with ? against\\nbaselines on all tested datasets. QE refers to query expansion by\\ndiffusion [12].\\nOS.EGM, where further uniform region sampling at 2 scales\\nis performed within each detected region. We refer to this\\nas OS.EGM-4. All methods are tested with k-NN search\\nand global diffusion [12], which is a method for query ex-\\npansion or manifold search and is known to significantly\\nimprove performance. Results are given in Table 1.\\nFS.EGM improves performance compared to uniform\\nsampling by focusing on salient objects. However, salient\\nobjects are not necessarily relevant for the particular dataset.\\nThis is what OS.EGM captures and boosts the search perfor-\\nmance, especially on Instre. On all datasets, MAC is better\\nthan uniform sampling (R-MAC). This is known to be due\\nto the fact that the network used [29] is directly fine-tuned to\\noptimize MAC. However, when using diffusion, we outper-\\nform it on all datasets. This can be explained by the fact that\\ndiffusion boosts any items that are similar to the top-ranking\\nones according to the original similarity [12], so it is essen-\\ntial that these items are reliable. A global descriptor is af-\\nfected by clutter in general. By contrast, our representation\\nis global yet clutter-free. Our improvements are larger on\\nInstre, which is more challenging due to small objects and\\nsevere background clutter. This is exactly where our detec-\\ntion is essential. Most Instre images are also quite different\\nthan the building images which the network is fine-tuned\\non. This is probably why our representations outperform\\nMAC even without diffusion on this dataset.\\nThere are several other previous approaches that deal\\nwith region detection or saliency masks, which are not di-\\nrectly comparable, so they are not included in Table 1. Nev-\\nertheless, we outperform their reported results. Salvador\\net al. [31] use the off-the-shelf VGG and fine-tune RPN in\\nthe test set. When not using query expansion, they obtain\\n71.0 in Oxford5k. Similarly, Jimenez et al. [15] learn class\\nweights and apply them on the activation maps of off-the-\\nshelf VGG and achieve 73.6 in Oxford5k. Song et al. [36]\\n1 3 5 10 21\\n70\\n80\\n90\\n15ms\\n30ms\\n50ms\\nregions per image\\nm\\nA\\nP\\nR-Match + QE\\nR-Match\\nFigure 10. mAP comparison of our global OS.EGM (?) to R-Match\\nwith uniformly sampled regional descriptors, with and without dif-\\nfusion on Oxford5k. Text labels refer to query time.\\ntrain on different datasets, and achieve 78.3 in Oxford5k.\\nThe results obtained by learning a saliency mask are not\\ncomparable since spatial verification with local features is\\nalways applied in the end [24]. Finally, Zheng et al. [43]\\nachieve 83.4 with regional representation on Oxford5k .\\nThey employ both CNN and local features, while we only\\nrely on CNN and much more compact representation. Fi-\\nnally, no work other than [12] evaluates on Instre which is\\nrather challenging due to small objects.\\nRegion cross-matching methods [30] represent an image\\nwith multiple vectors, sacrificing memory footprint and\\ncomplexity for accuracy. In particular, the memory is linear\\nin the number of regions, while the complexity is quadratic.\\nWe compare our global representation with region cross-\\nmatching (R-Match) and regional diffusion [12] in Fig-\\nure 10. Different numbers of regions are obtained by GMM\\nreduction, exactly as in [12].\\nCompared to regional descriptors, we require about 4\\ntimes less memory to achieve the same performance. The\\nruntime complexity gain is in the order of 42, which holds\\nfor the case of R-Match and also for the first part of dif-\\nfusion where Euclidean nearest neighbors are found. The\\ndiffusion complexity is O(m), where m is the number of\\nnon-zero entries of the graph. We found that m is 3.7 times\\nsmaller in our case and our measurements of actual query\\ntimings agree with this ratio.\\n5. Conclusions\\nWe propose a region detection approach that is dataset\\nspecific but requires no supervision. It captures not only\\nsalient objects by considering each image individually but\\nalso frequently appearing ones by considering the dataset\\nas a whole. As a result, we avoid separate indexing of re-\\ngional descriptors and construct a global descriptor by pool-\\ning over data-dependent regions, which performs well under\\nbackground clutter and severe occlusions. We demonstrate\\nthat this approach is effective in particular object retrieval\\nwhere background clutter is a common problem.\\nReferences\\n[1] R. Arandjelovic´ and A. Zisserman. Visual vocabulary with a\\nsemantic twist. In ACCV, 2014. 2\\n[2] Y. Avrithis and Y. Kalantidis. Approximate gaussian mix-\\ntures for large scale vocabularies. In ECCV, pages 15–28.\\nSpringer, 2012. 3, 4\\n[3] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, and\\nS. Carlsson. From generic to specific deep representations\\nfor visual recognition. arXiv preprint arXiv:1406.5774,\\n2014. 2, 5\\n[4] A. Babenko and V. Lempitsky. Aggregating deep convolu-\\ntional features for image retrieval. In ICCV, 2015. 1, 4\\n[5] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky.\\nNeural codes for image retrieval. In ECCV, 2014. 1\\n[6] O. Chum and J. Matas. Unsupervised discovery of co-\\noccurrence in sparse high dimensional data. In CVPR, June\\n2010. 2\\n[7] S. Gammeter, L. Bossard, T. Quack, and L. V. Gool. I know\\nwhat you did last summer: Object-level auto-annotation of\\nholiday snaps. In ICCV, 2009. 2\\n[8] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep image\\nretrieval: Learning global representations for image search.\\nECCV, 2016. 1, 5, 6\\n[9] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. End-to-end\\nlearning of deep visual representations for image retrieval.\\narXiv preprint arXiv:1610.07940, 2016. 2\\n[10] C. H. Hubbell. An input-output approach to clique identifi-\\ncation. Sociometry, 1965. 5\\n[11] A. Iscen, Y. Avrithis, G. Tolias, T. Furon, and O. Chum. Fast\\nspectral ranking for similarity search. arXiv, 2017. 1, 5\\n[12] A. Iscen, G. Tolias, Y. Avrithis, T. Furon, and O. Chum. Ef-\\nficient diffusion on region manifolds: Recovering small ob-\\njects with compact cnn representations. In CVPR, 2017. 1,\\n2, 5, 6, 8\\n[13] H. Je´gou, M. Douze, and C. Schmid. Improving bag-of-\\nfeatures for large scale image search. IJCV, 87(3):316–336,\\nFebruary 2010. 2\\n[14] D.-j. Jeong, S. Choo, W. Seo, and N. I. Cho. Regional deep\\nfeature aggregation for image retrieval. In ICASSP, 2017. 2\\n[15] A. Jimenez, J. M. Alvarez, and X. Giro-i Nieto. Class-\\nweighted convolutional features for visual instance search.\\nBMVC, 2017. 2, 8\\n[16] H. Jin Kim, E. Dunn, and J.-M. Frahm. Learned contextual\\nfeature reweighting for image geo-localization. In CVPR,\\n2017. 2\\n[17] Y. Kalantidis, C. Mellina, and S. Osindero. Cross-\\ndimensional weighting for aggregated deep convolutional\\nfeatures. In arXiv, 2015. 1, 2, 3, 5, 6\\n[18] L. Katz. A new status index derived from sociometric analy-\\nsis. Psychometrika, 18(1):39–43, 1953. 5\\n[19] J. Knopp, J. Sivic, and T. Pajdla. Avoiding confusing features\\nin place recognition. In ECCV, 2010. 2\\n[20] Z. Laskar and J. Kannala. Context aware query image rep-\\nresentation for particular object retrieval. In Scandinavian\\nConference on Image Analysis, 2017. 2\\n[21] N. MEJ. Networks: an introduction. Oxford University\\nPress, Oxford, 2010. 5\\n[22] K. Mikolajczyk and J. Matas. Improving descriptors for fast\\ntree matching by optimal linear projection. In CVPR, 2007.\\n5\\n[23] J. Nocedal and S. Wright. Numerical optimization. Springer,\\n2006. 5\\n[24] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han. Large-\\nscale image retrieval with attentive deep local features. In\\narXiv, 2016. 1, 2, 8\\n[25] A. Oliva and A. Torralba. Building the gist of a scene: The\\nrole of global image features in recognition. Progress in\\nbrain research, 155:23–36, 2006. 1\\n[26] D. Omercevic, R. Perko, A. T. Targhi, J.-O. Eklundh, and\\nA. Leonardis. Vegetation segmentation for boosting perfor-\\nmance of mser feature detector. In Computer Vision Winter\\nWorkshop, 2008. 2\\n[27] L. Page, S. Brin, R. Motwani, and T. Winograd. The PageR-\\nank citation ranking: bringing order to the web. 1999. 5\\n[28] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisser-\\nman. Object retrieval with large vocabularies and fast spatial\\nmatching. In CVPR, June 2007. 1, 2, 6\\n[29] F. Radenovic´, G. Tolias, and O. Chum. CNN image retrieval\\nlearns from bow: Unsupervised fine-tuning with hard exam-\\nples. ECCV, 2016. 2, 5, 6, 8\\n[30] A. S. Razavian, J. Sullivan, S. Carlsson, and A. Maki. Vi-\\nsual instance retrieval with deep convolutional networks. ITE\\nTransactions on Media Technology and Applications, 4:251–\\n258, 2016. 1, 2, 8\\n[31] A. Salvador, X. Giro´-i Nieto, F. Marque´s, and S. Satoh.\\nFaster r-cnn features for instance search. In CVPRW, 2016.\\n1, 2, 8\\n[32] R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell,\\nD. Parikh, and D. Batra. Grad-CAM: Why did you say that?\\nvisual explanations from deep networks via gradient-based\\nlocalization. arXiv preprint arXiv:1610.02391, 2016. 3\\n[33] M. Shi, Y. Avrithis, and H. Jegou. Early burst detection for\\nmemory-efficient image retrieval. In CVPR, 2015. 2\\n[34] K. Simonyan and A. Zisserman. Very deep convolutional\\nnetworks for large-scale image recognition. ICLR, 2014. 6\\n[35] J. Sivic and A. Zisserman. Video Google: A text retrieval\\napproach to object matching in videos. In ICCV, 2003. 1, 2\\n[36] J. Song, T. He, L. Gao, X. Xu, and H. T. Shen. Deep region\\nhashing for efficient large-scale instance search from images.\\nIn arXiv, 2017. 1, 2, 8\\n[37] G. Tolias, Y. Avrithis, and H. Je´gou. Image search with se-\\nlective match kernels: aggregation across single and multiple\\nimages. IJCV, 2016. 2\\n[38] G. Tolias, Y. Kalantidis, and Y. Avrithis. Symcity: Feature\\nselection by symmetry for large scale image retrieval. In\\nACM Multimedia, 2012. 2\\n[39] G. Tolias, R. Sicre, and H. Je´gou. Particular object retrieval\\nwith integral max-pooling of cnn activations. ICLR, 2016. 1,\\n2, 5, 6, 8\\n[40] P. Turcot and D. G. Lowe. Better matching with fewer\\nfeatures: The selection of useful features in large database\\nrecognition problems. In ICCVW, 2009. 2\\n[41] S. Vigna. Spectral ranking. arXiv preprint arXiv:0912.0238,\\n2009. 5\\n[42] S. Wang and S. Jiang. Instre: a new benchmark for instance-\\nlevel object retrieval and recognition. ACM Transactions on\\nMultimedia Computing, Communications, and Applications\\n(TOMM), 11:37, 2015. 6\\n[43] L. Zheng, S. Wang, J. Wang, and Q. Tian. Accurate im-\\nage search with multi-scale contextual evidences. IJCV,\\n120(1):1–13, 2016. 8\\n[44] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-\\nralba. Learning deep features for discriminative localization.\\nIn cvpr, June 2016. 3\\n[45] D. Zhou, J. Weston, A. Gretton, O. Bousquet, and\\nB. Scho¨lkopf. Ranking on data manifolds. In NIPS, 2003. 5\\n',\n",
       " 'fulltextIdentifier': None,\n",
       " 'id': 93945852,\n",
       " 'identifiers': ['oai:arXiv.org:1709.04725', None],\n",
       " 'language': None,\n",
       " 'num_record': 24524,\n",
       " 'oai': 'oai:arXiv.org:1709.04725',\n",
       " 'relations': [],\n",
       " 'repo_id': 144,\n",
       " 'subjects': ['text'],\n",
       " 'title': 'Unsupervised object discovery for instance recognition',\n",
       " 'topics': ['Computer Science - Computer Vision and Pattern Recognition'],\n",
       " 'types': [],\n",
       " 'year': 2018}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_full[article]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/pdf/1709.04725.pdf'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://arxiv.org/pdf/'\n",
    "url + articles_full[article]['identifiers'][0][14:] + '.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
