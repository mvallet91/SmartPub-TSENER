{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term and Sentence Expansion for the training of a NER Tagger\n",
    "\n",
    "In the first pipeline we go over the preparation required for the TSE-NER, from data collection, extraction (to MongoDB), indexing (to Elasticsearch) and other preliminary steps (word2vec and doc2vec models). \n",
    "\n",
    "In this second pipeline we lay out the steps of Term Expansion, Sentence Expansion, NER Training, and NER Tagging. In short, the steps are:\n",
    "\n",
    "1. Initial Data Generation\n",
    "2. Term Expansion\n",
    "3. Sentence Expansion\n",
    "4. Training Data Generation\n",
    "5. Train NER Tagger\n",
    "6. Extract new entities\n",
    "7. Filtering\n",
    "\n",
    "The papers collected are expendable at this point, but the Elasticsearch indexes and embedding models are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is in case you update modules while working\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Configuration\n",
    "\n",
    "Before we start, this process requires certain high-level configuration parameters that can be introduced below (you can also dive into the code, we are trying to make it as clear as possible for further customization).\n",
    "\n",
    "* model_name: a one-word name for the model, it should be representative of the facet that the model focuses on\n",
    "* seeds: list (in format [‘item’, ‘item’ ... ]) of representative entities of the type\n",
    "* context_words: list (in format [‘item’, ‘item’ ... ]) of words that are usually surrounding the entities, that often appear in sentences together (only required when PMI filtering is applied)\n",
    "* sentence_expansion: True or False if the sentence expansion step should be performed (term expansion is always done) \n",
    "* training_cycles: number of the training cycles to perform\n",
    "* filtering_pmi: True or False if Pointwise Mutual Information filtering should be used at the end of each cycle\n",
    "* filtering_st: True or False if Similarity\tTerms filtering should be used at the end of each cycle\n",
    "* filtering_ws: True or False if Stopword + WordNet filtering should be used at the end of each cycle\n",
    "* filtering_kbl: True or False if Knowledge Base Lookup filtering should be used at the end of each cycle\n",
    "\n",
    "For example, we can provide with a number of entities of the **dataset** type, which is also the name of our model. In this context, datasets are collections of information that were constructed with a specific structure and a purpose, such as comparing performance of different technologies in the same task.\n",
    "\n",
    "For example, 50 entities of the dataset facet with the rest of initial configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'dataset_50'\n",
    "\n",
    "seeds = ['buzzfeed', 'pslnl', 'dailymed', 'robust04', 'scovo', 'ask.com', 'cacm', 'stanford large network dataset', \n",
    "         'mediaeval', 'lexvo', 'spambase', 'shop.com', 'orkut', 'jnlpba', 'cyworld', 'citebase', 'blog06', 'worldcat', \n",
    "         'booking.com', 'semeval', 'imagenet', 'nasdaq', 'brightkite', 'movierating', 'webkb', 'ionosphere', 'moviepilot', \n",
    "         'duc2001', 'datahub', 'cifar', 'tdt', 'refseq', 'stack overflow', 'wikiwars', 'blogpulse', 'ws-353', 'gerbil', \n",
    "         'wikia', 'reddit', 'ldoce', 'kitti dataset', 'specweb', 'fedweb', 'wt2g', 'as3ap', 'friendfeed', 'new york times', \n",
    "         'chemid', 'imageclef', 'newegg']\n",
    "\n",
    "context_words = ['dataset', 'corpus', 'collection', 'repository', 'benchmark']\n",
    "sentence_expansion = True\n",
    "training_cycles = 5\n",
    "filtering_pmi = True\n",
    "filtering_st = True\n",
    "filtering_ws = True\n",
    "filtering_kbl = True\n",
    "filtering_majority = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important** In addition to this configuration, we need to find the `config.py` file and edit the ROOTPATH and STANFORD_NER_PATH to the respective locations! In that file we can also edit the ports used for Elasticsearch.\n",
    "\n",
    "We also import all the scripts required for the process, as mentioned before, you can check he code for further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from m1_preprocessing import seed_data_extraction, term_sentence_expansion, training_data_generation, ner_training\n",
    "from m1_postprocessing import extract_new_entities, filtering\n",
    "import config as cfg\n",
    "import gensim\n",
    "import elasticsearch\n",
    "import time\n",
    "import re \n",
    "import string\n",
    "\n",
    "doc2vec_model = gensim.models.Doc2Vec.load('embedding_models/doc2vec.model') #this is the path of the model created in the previous pipeline\n",
    "es = elasticsearch.Elasticsearch([{'host': 'localhost', 'port': 9200}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its creation, TSE-NER was imagined as a cyclic process, where we generate training data, train the NER model, extract the entities from our full-corpus and then use those as the new seeds (filtering out as much noise as possible, of course). However, in this demo we will go step by step for a single cycle, and at the end show how it would look like for a cyclic process.\n",
    "\n",
    "Therefore, we create a new variable that should iterate but will be fixed for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cycle = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSE-NER Process\n",
    "### 1 - Initial Data Generation\n",
    "\n",
    "Once we have the seeds and basic configuration, the first step consists of searching our entire corpus for sentences that contain the seed terms. This will create a txt file with the seeds, and one with the sentences in the `processing_files` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started initial training data extraction\n",
      "Extracting sentences for 50 seed terms\n",
      "..................................................Process finished with 50 seeds and 1979 sentences added for training in cycle number 0\n"
     ]
    }
   ],
   "source": [
    "seed_data_extraction.sentence_extraction(model_name, cycle, seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Term Expansion\n",
    "For the *Term Expansion* we process all the sentences we just extracted, and use the Natural Language Toolkit (NLTK) to find generic entities, these are words that could potentially be an entity given that they are nouns, they have a certain position in a sentence, and/or a relationship with other parts of the sentence, more information [here](https://www.nltk.org/book/ch07.html). \n",
    "\n",
    "Then we use the vectors of those words, obtained from word2vec, and cluster them using k-means, selecting the best number of clusters in base of their silhouette score. If there is a seed entity in the cluster, we consider that the rest of the potential entities in that same cluster should be kept as Expanded Terms. Like the previous step, this creates a txt file with the expanded terms in the `processing_files` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting term expansion\n",
      "Started to extract generic named entity from sentences...\n",
      "....Finished processing sentences with 4161 new possible entities\n",
      "Started term clustering\n",
      "Added 160 expanded terms\n"
     ]
    }
   ],
   "source": [
    "term_sentence_expansion.term_expansion(model_name, cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK produces over 4 thousand potential entities, after clustering and selecting, we keep 160 Expanded Terms.\n",
    "\n",
    "### 3 - Sentence Expansion\n",
    "In *Sentence Expansion*, we use doc2vec to find a single similar sentence (this can be modified in the code, for instance, in line 248) to each one of the sentences that we obtained in Step 1. If the sentence has a consine similarity above 0.5 (also can be changed), we add it to our set of Expanded Sentences. This set is stored in the `processing_files`.\n",
    "\n",
    "*Note:* There is a chance that this process runs out of memmory if the doc2vec model is too large, this is because it needs to compare the current sentence against ALL other sentences to find the most similar. To fix this, you might have to retrace your steps and create a smaller model in the Preparation Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentence expansion\n",
      "Finding similar sentences to the 3115 starting sentences\n",
      "....Added 130 expanded sentences to the 3115 original\n"
     ]
    }
   ],
   "source": [
    "term_sentence_expansion.sentence_expansion(model_name, cycle, doc2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Expansion generates **130** new sentences, if they include one of the Expanded Terms, they will be used as possitive examples for the training, if they do not include any entity, they are also very helpful as similar sentences but as negative examples. We argue that this helps to improve the performance of the NER Tagger.\n",
    "\n",
    "### 4 - Training Data Generation\n",
    "For the [Stanford NER Tagger](https://nlp.stanford.edu/software/CRF-NER.html) model training, a specific file is required. The format consists of sentences, compiled as a list of *word -> label*, where entities are labelled either with the current entity, say **DATASET**, or with **O** if they are not. \n",
    "\n",
    "    ...\n",
    "    we     O\n",
    "    apply  O\n",
    "    this   O\n",
    "    to     O\n",
    "    the    O\n",
    "    Wikia  DATASET\n",
    "    corpus O\n",
    "    ...\n",
    "    \n",
    "For this, we take all the Sentences + Expanded Sentences, and label all the Seed Terms + Expanded Terms in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling sentences in the required format\n",
      "3285 lines labelled\n"
     ]
    }
   ],
   "source": [
    "training_data_generation.sentence_labelling(model_name, cycle, sentence_expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - NER Tagger Training\n",
    "Once we have the file with all the labelled sentences, we have to create a [property file](https://nlp.stanford.edu/software/crf-faq.shtml#a) for the Tagger. In this file we can edit certain configurations, point to the correct training file, and the location of the Tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating property file for Stanford NER training\n"
     ]
    }
   ],
   "source": [
    "ner_training.create_prop(model_name, cycle, sentence_expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data in place, and the property file ready, we can start training. This script executes a Java process like command line, which will generate a CRF (Conditional Random Field) file: the NER Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    }
   ],
   "source": [
    "ner_training.train_model(model_name, cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have a Long-Tail Entity Extraction Model!\n",
    "\n",
    "### 6 - Extract New Entities\n",
    "Since this is the goal of this whole process, we will take it step by step to see what's happening in the new Entity Extraction. Python fortunately allows for very easy use of the model we trained.\n",
    "\n",
    "First, we instantiate the Tagger like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.corpus import stopwords\n",
    "path_to_model = 'crf_trained_files/dataset_50_TSE_model_0.ser.gz' \n",
    "STANFORD_NER_PATH = 'stanford_files/stanford-ner.jar' # This should be in config, but we can show it again\n",
    "ner_tagger = StanfordNERTagger(path_to_model, STANFORD_NER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example sentence from one of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entities(text):\n",
    "    tagged = ner_tagger.tag(text_to_tag.split())\n",
    "    result = []\n",
    "    for jj, (a, b) in enumerate(tagged):\n",
    "        no_tag = 'O'\n",
    "        if b != no_tag:\n",
    "            a = a.translate(str.maketrans('', '', string.punctuation))\n",
    "            try:\n",
    "                if res[jj + 1][1] != no_tag:\n",
    "                    temp = res[jj + 1][0].translate(str.maketrans('', '', string.punctuation))\n",
    "                    bigram = a + ' ' + temp\n",
    "                    result.append(bigram)\n",
    "            except KeyError:\n",
    "                result.append(a)\n",
    "    extracted_words = [word for word in set(result) if word not in stopwords.words('english')]\n",
    "    return extracted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This suggests that training a model to rerank responses based on labeled Reddit threads and responses cannot help improve performance. Even though the logistic regression model did improve the appropriateness of responses selected for Reddit threads, VHREDRedditPolitics is used extremely rarely in the final system see Section 4.\n",
      "['Reddit']\n"
     ]
    }
   ],
   "source": [
    "text_to_tag = \"This suggests that training a model to rerank responses based on labeled Reddit threads and responses cannot help improve performance. Even though the logistic regression model did improve the appropriateness of responses selected for Reddit threads, VHREDRedditPolitics is used extremely rarely in the final system see Section 4.\"\n",
    "extracted_entities = get_entities(text_to_tag)\n",
    "print(text_to_tag)\n",
    "print(extracted_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tagger works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We propose a rather straightforward pipeline combining deep-feature extraction using a CNN pretrained on ImageNet and a classic clustering algorithm to classify sets of images. We study the impact of different pretrained CNN feature extractors on the problem of image set clustering for object classification as well as fine-grained classification.\n",
      "['problem', 'ImageNet']\n"
     ]
    }
   ],
   "source": [
    "text_to_tag = \"We propose a rather straightforward pipeline combining deep-feature extraction using a CNN pretrained on ImageNet and a classic clustering algorithm to classify sets of images. We study the impact of different pretrained CNN feature extractors on the problem of image set clustering for object classification as well as fine-grained classification.\"\n",
    "extracted_entities = get_entities(text_to_tag)\n",
    "print(text_to_tag)\n",
    "print(extracted_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we got some noise, so we can check our filtering strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'processing_files/' + model_name + '_extracted_entities_' + str(cycle) + '.txt'\n",
    "f1 = open(path, 'w', encoding='utf-8')\n",
    "for item in extracted_entities:\n",
    "    f1.write(item + '\\n')\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could filter these new entities and start the process all over again with a larger set of seed terms.\n",
    "\n",
    "### Filtering\n",
    "\n",
    "Here we can apply different filters and evaluate the results. \n",
    "\n",
    "* WS\n",
    "WordNet + Stopword filtering simply filters out stop and common words, following the assumption that long-tail entities may be rare and domain specific. \n",
    "\n",
    "* ST\n",
    "Similar Terms filtering is based on the same approach as the Term Expansion, by clustering the vectors of the terms and only keeping those clusters where there is one of the original seed terms.\n",
    "\n",
    "* PMI\n",
    "Pointwise Mutual Information (PMI) filtering adopts a semantic similarity measure derived  from the number of times two given keywords appear together in a sentence in our corpus   (for example, the sentence., \"we evaluate on x\" typically indicates a dataset). A set of context words, terms that often appear with the entities in the same sentence, is required for this filtering.\n",
    "\n",
    "* KBL\n",
    "Knowledge Base Lookup, like WordNet filtering, follows the assumption that long-tail entities will not appear in a common knowledge database, such as DBpedia.\n",
    "\n",
    "* Ensemble\n",
    "To reduce the amount of false positives at the end of the process, we propose to only keep entities that remain after applying several, or all, filtering approaches to the results. In the current implementation, the resulting entities have to pass all filters.\n",
    "\n",
    "For more details about the filtering, please refer to the main article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar Term filtering relies on clustering of the vectors of the extracted entities, \n",
    "# and therefore doesn't work with the few entities of this example\n",
    "\n",
    "filtering.filter_st(model_name, cycle, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['problem', 'ImageNet']\n"
     ]
    }
   ],
   "source": [
    "print(extracted_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 2 entities with PMI\n",
      "2 entities are kept from the total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imagenet', 'problem']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering.filter_pmi(model_name, cycle, context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 2 entities with WordNet and Stopwords\n",
      "1 entities are kept from the total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imagenet']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering.filter_ws(model_name, cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 2 entities with knowledge base lookup\n",
      "2 entities are kept from the total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imagenet', 'problem']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering.filter_kbl(model_name, cycle, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 2 entities by vote of selected filter methods\n",
      "1 entities are kept from the total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imagenet']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering.majority_vote(model_name, cycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering we get the actual dataset for the example!\n",
    "\n",
    "## Full-text Tagging\n",
    "\n",
    "For a more extended use, we can apply it to the document in our corpus. First we define a cleaning function to get rid of some characters that can affect the tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(es_doc):\n",
    "    content = doc[\"_source\"][\"content\"]\n",
    "    content = content.replace(\"@ BULLET\", \"\")\n",
    "    content = content.replace(\"@BULLET\", \"\")\n",
    "    content = content.replace(\", \", \" , \")\n",
    "    content = content.replace('(', '')\n",
    "    content = content.replace(')', '')\n",
    "    content = content.replace('[', '')\n",
    "    content = content.replace(']', '')\n",
    "    content = content.replace(',', ' ,')\n",
    "    content = content.replace('?', ' ?')\n",
    "    content = content.replace('..', '.')\n",
    "    content = re.sub(r\"(\\.)([A-Z])\", r\"\\1 \\2\", content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we can take some text to tag, for instance, we can search for some documents by their title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 27 Hits:\n",
      "arxiv_141535856 Towards Practical Verification of Machine Learning: The Case of Computer\n",
      "  Vision Systems\n",
      "arxiv_83836033 Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social\n",
      "  Media\n",
      "arxiv_84327993 Compiling LATEX to computer algebra-enabled HTML5\n",
      "arxiv_129361451 Neural Networks Architecture Evaluation in a Quantum Computer\n",
      "arxiv_86419363 Robust Computer Algebra, Theorem Proving, and Oracle AI\n",
      "arxiv_83844865 Aligned Image-Word Representations Improve Inductive Transfer Across\n",
      "  Vision-Language Tasks\n",
      "arxiv_141535714 Discriminant Projection Representation-based Classification for Vision\n",
      "  Recognition\n"
     ]
    }
   ],
   "source": [
    "res = es.search(index = \"ir\", body = {\"query\": {\"match\": {\"title\" : \"computer vision\"}}}, size = 7)\n",
    "\n",
    "print(\"Got %d Hits:\" % res['hits']['total'])\n",
    "for doc in res['hits']['hits']:\n",
    "    print(doc['_id'], doc['_source']['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the full corpus and labell all entities, evaluate performance, and improve for next training cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv_141535856 Towards Practical Verification of Machine Learning: The Case of Computer\n",
      "  Vision Systems\n",
      "['Scale', 'arXiv161002357', 'MobileNet', 'arXiv', 'Recognition', 'arXiv160407316', 'Imagenet', 'ResNet50', 'corresponding', 'ImageNet', 'problem', 'correctness', 'correspond', 'DNNs', 'Journal', 'imagenet', 'arXiv170808559', 'International', 'MNIST', 'convolutional', 'Visual', 'IMAGENET', 'IEEE', 'HOcclMask']\n",
      "\n",
      "arxiv_83836033 Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social\n",
      "  Media\n",
      "['corresponding', 'convolutional', 'correspond', 'Reddit']\n",
      "\n",
      "arxiv_84327993 Compiling LATEX to computer algebra-enabled HTML5\n",
      "[]\n",
      "\n",
      "arxiv_129361451 Neural Networks Architecture Evaluation in a Quantum Computer\n",
      "['probability', 'Journal', 'arXiv', 'problems', 'arXiv14123489', 'IEEE', 'arXiv170401127', 'International']\n",
      "\n",
      "arxiv_86419363 Robust Computer Algebra, Theorem Proving, and Oracle AI\n",
      "['problem', 'problems', 'ITPs', 'probabilities', 'corresponding', 'GitHub']\n",
      "\n",
      "arxiv_83844865 Aligned Image-Word Representations Improve Inductive Transfer Across\n",
      "  Vision-Language Tasks\n",
      "['ResNet', 'problem', 'ResNet152', 'Resnet', 'correct', 'problems', 'regionR', 'Top20', 'Resnet50', 'corresponding', 'correctly', 'ImageNet']\n",
      "\n",
      "arxiv_141535714 Discriminant Projection Representation-based Classification for Vision\n",
      "  Recognition\n",
      "['Ucf50', 'problem', 'corresponding']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in res['hits']['hits']:\n",
    "    print(doc['_id'], doc['_source']['title'])\n",
    "    text_to_tag = clean_text(doc)\n",
    "    print(get_entities(text_to_tag))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the filtering approaches that we showed before, we can obtain the datasets used in each paper.\n",
    "\n",
    "This is only a demo of the TSE-NER approach, with a larger dataset we can improve the recall of the Tagger and extract even more entities, maintaining the precision with proper filtering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
